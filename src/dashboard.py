import json
import os
from pathlib import Path

import pandas as pd
import plotly.express as px
import streamlit as st

from src.run_utils import latest_run_dir

ART_DIR = Path("artifacts")

st.set_page_config(page_title="Network KPI Anomaly Dashboard", layout="wide")
st.title("Network KPI Anomaly Dashboard")

st.markdown(
    """
This dashboard visualizes anomaly scores and alerts generated by the trained model.
Recommended flow:
1) Run tuning: `python -m src.timecv_tune_gb`
2) (Optional) Run eval: `python -m src.evaluate`
3) Start dashboard: `python -m streamlit run src/dashboard.py`
"""
)


@st.cache_data
def load_table(path_parquet: str | None, path_csv: str | None) -> pd.DataFrame:
    if path_parquet and os.path.exists(path_parquet):
        return pd.read_parquet(path_parquet)
    if path_csv and os.path.exists(path_csv):
        return pd.read_csv(path_csv)
    raise FileNotFoundError("Scored file not found.")


def latest_by_glob(pattern: str) -> Path | None:
    runs = sorted(ART_DIR.glob(pattern))
    return runs[-1] if runs else None


def scored_paths_for(source_key: str, run_dir: Path) -> tuple[str | None, str | None, str | None]:
    if source_key == "tuned_gb":
        scored_path = str(run_dir / "scored_labeled.parquet")
        csv_path = str(run_dir / "scored_labeled.csv")
        inc_path = None
    elif source_key == "timecv_models":
        scored_path = str(run_dir / "scored_labeled.parquet")
        csv_path = str(run_dir / "scored_labeled.csv")
        inc_path = None
    elif source_key == "train":
        scored_path = str(run_dir / "scored_unlabeled.parquet")
        csv_path = str(run_dir / "scored_unlabeled.csv")
        inc_path = str(run_dir / "incidents_unlabeled.parquet")
    else:
        scored_path = str(run_dir / "scored_labeled.parquet")
        csv_path = str(run_dir / "scored_labeled.csv")
        inc_path = str(run_dir / "incidents_labeled.parquet")
    return scored_path, csv_path, inc_path


def resolve_source() -> tuple[str, Path, str | None, str | None, str | None] | None:
    tuned_run = latest_by_glob("run_timecv_tune_gb_*")
    models_run = latest_by_glob("run_timecv_models_*")
    train_run = latest_run_dir("train")
    eval_run = latest_run_dir("eval")

    priorities = [
        ("tuned_gb", tuned_run),
        ("timecv_models", models_run),
        ("train", train_run),
        ("eval", eval_run),
    ]

    for key, run_dir in priorities:
        if run_dir is None:
            continue
        scored_path, csv_path, inc_path = scored_paths_for(key, run_dir)
        if (scored_path and os.path.exists(scored_path)) or (csv_path and os.path.exists(csv_path)):
            return key, run_dir, scored_path, csv_path, inc_path
    return None


def load_json(path: Path) -> dict | None:
    if path.exists():
        return json.loads(path.read_text(encoding="utf-8"))
    return None


def resolve_incidents() -> tuple[Path | None, str | None, str | None]:
    eval_run = latest_run_dir("eval")
    train_run = latest_run_dir("train")
    if eval_run:
        inc_path = eval_run / "incidents_labeled.parquet"
        csv_path = eval_run / "incidents_labeled.csv"
        return eval_run, str(inc_path), str(csv_path)
    if train_run:
        inc_path = train_run / "incidents_unlabeled.parquet"
        csv_path = train_run / "incidents_unlabeled.csv"
        return train_run, str(inc_path), str(csv_path)
    return None, None, None


tuned_run = latest_by_glob("run_timecv_tune_gb_*")
tuned_best = None
tuned_ops = None
if tuned_run:
    tuned_best = load_json(tuned_run / "best_config.json")
    tuned_ops = load_json(tuned_run / "operating_points.json")

resolved = resolve_source()
if resolved is None:
    st.warning("No scored artifacts found. Run tuning or training to generate scored files.")
    st.stop()

source_key, run_dir, scored_path, csv_path, inc_path = resolved
source_labels = {
    "tuned_gb": "tuned GB (timecv_tune_gb)",
    "timecv_models": "model comparison (timecv_models)",
    "train": "train run",
    "eval": "eval run",
}

st.caption(f"Data source: {source_labels.get(source_key, source_key)} | {run_dir}")

try:
    df = load_table(scored_path, csv_path)
except FileNotFoundError:
    st.warning("Scored file missing for the selected run.")
    st.stop()

if "timestamp" in df.columns:
    df["timestamp"] = pd.to_datetime(df["timestamp"], errors="coerce", utc=True)
    df = df.dropna(subset=["timestamp"]).sort_values("timestamp").reset_index(drop=True)
else:
    df["timestamp"] = df.index

if "anomaly_score" not in df.columns and "score" in df.columns:
    df["anomaly_score"] = df["score"]

if "anomaly_score" not in df.columns:
    st.warning("No anomaly_score column found in scored data.")
    st.stop()

selected_threshold = None

with st.sidebar:
    st.header("Controls")
    st.caption(f"Rows: {len(df):,}")

    if tuned_ops:
        preferred = ["f2_opt", "high_recall", "alert_budget_5p"]
        op_keys = [k for k in preferred if k in tuned_ops] + [k for k in tuned_ops.keys() if k not in preferred]
        op_choice = st.selectbox("Operating point", op_keys, index=0)
        selected_threshold = float(tuned_ops[op_choice]["threshold"])

        if tuned_best:
            st.subheader("Tuned GB")
            st.json(
                {
                    "n_estimators": int(tuned_best.get("n_estimators", 0)),
                    "learning_rate": float(tuned_best.get("learning_rate", 0.0)),
                    "max_depth": int(tuned_best.get("max_depth", 0)),
                    "min_samples_leaf": int(tuned_best.get("min_samples_leaf", 0)),
                    "subsample": float(tuned_best.get("subsample", 0.0)),
                }
            )

    if pd.api.types.is_datetime64_any_dtype(df["timestamp"]):
        tmin, tmax = df["timestamp"].min(), df["timestamp"].max()
        start, end = st.slider(
            "Time range",
            min_value=tmin.to_pydatetime(),
            max_value=tmax.to_pydatetime(),
            value=(tmin.to_pydatetime(), tmax.to_pydatetime()),
        )
    else:
        start, end = None, None
        st.caption("Time range slider disabled (no timestamps)")

    show_alerts_only = st.checkbox("Show alerts only", value=False)

if selected_threshold is not None:
    df["is_alert"] = df["anomaly_score"] >= selected_threshold
elif "is_alert" not in df.columns and "pred_label" in df.columns:
    df["is_alert"] = df["pred_label"].astype(bool)
elif "is_alert" not in df.columns:
    df["is_alert"] = False

if start and end:
    start_utc = pd.Timestamp(start).tz_convert("UTC") if pd.Timestamp(start).tzinfo else pd.Timestamp(start).tz_localize("UTC")
    end_utc = pd.Timestamp(end).tz_convert("UTC") if pd.Timestamp(end).tzinfo else pd.Timestamp(end).tz_localize("UTC")
    mask = (df["timestamp"] >= start_utc) & (df["timestamp"] <= end_utc)
else:
    mask = pd.Series([True] * len(df), index=df.index)

d = df.loc[mask].copy()
if show_alerts_only:
    d = d[d["is_alert"] == True]

colA, colB = st.columns([2, 1])

with colA:
    fig = px.line(d, x="timestamp", y="anomaly_score", title="Anomaly Score (higher = more anomalous)")
    alerts = d[d["is_alert"] == True]
    if not alerts.empty:
        fig2 = px.scatter(alerts, x="timestamp", y="anomaly_score")
        for tr in fig2.data:
            fig.add_trace(tr)
    st.plotly_chart(fig, use_container_width=True)

with colB:
    st.subheader("Alert Summary")
    st.metric("Alert rate", f"{(df['is_alert'].mean()*100):.2f}%")

    incident_source = None
    inc_parquet = inc_path
    inc_csv = inc_path.replace(".parquet", ".csv") if inc_path else None
    if not inc_parquet or not (os.path.exists(inc_parquet) or (inc_csv and os.path.exists(inc_csv))):
        incident_source, inc_parquet, inc_csv = resolve_incidents()

    if inc_parquet and (os.path.exists(inc_parquet) or (inc_csv and os.path.exists(inc_csv))):
        inc = load_table(inc_parquet, inc_csv)
        st.metric("Incident count", int(len(inc)))
        if incident_source and incident_source != run_dir:
            st.caption(f"Incident source: {incident_source}")
        st.dataframe(inc.tail(10), use_container_width=True)
    else:
        st.info("Incident file not found (run train/evaluate).")

st.subheader("KPI Overview")
kpi_candidates = [c for c in ["throughput", "congestion", "packet_loss", "latency", "jitter", "bandwidth"] if c in d.columns]
if not kpi_candidates:
    st.info("KPI columns not found in scored data.")
else:
    kpi = st.selectbox("Select KPI", kpi_candidates)
    figk = px.line(d, x="timestamp", y=kpi, title=f"KPI: {kpi}")
    st.plotly_chart(figk, use_container_width=True)

if "anomaly" in df.columns:
    st.subheader("Label vs Alerts (Labeled Data)")
    y_true = df["anomaly"].astype(int)
    y_pred = df["is_alert"].astype(int)
    tp = int(((y_true == 1) & (y_pred == 1)).sum())
    fp = int(((y_true == 0) & (y_pred == 1)).sum())
    fn = int(((y_true == 1) & (y_pred == 0)).sum())
    tn = int(((y_true == 0) & (y_pred == 0)).sum())
    st.write({"tp": tp, "fp": fp, "fn": fn, "tn": tn})
